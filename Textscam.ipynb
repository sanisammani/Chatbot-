{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPbqov3EuEDmVpTaf0dQWao",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanisammani/Chatbot-/blob/main/Textscam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_hNsWrdFNs5P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f1e4ece-4164-479f-bab0-8ee11f64f219"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset (update path if needed)\n",
        "file_path = '/content/scamdata.csv'  # Update this if the file path changes\n",
        "try:\n",
        "    scam_data = pd.read_csv(file_path)\n",
        "except FileNotFoundError:\n",
        "    raise FileNotFoundError(f\"File not found at {file_path}. Please check the path.\")\n",
        "\n",
        "# Ensure necessary columns exist in the dataset\n",
        "required_columns = ['input', 'output']\n",
        "missing_columns = [col for col in required_columns if col not in scam_data.columns]\n",
        "if missing_columns:\n",
        "    raise ValueError(f\"The dataset is missing required columns: {missing_columns}\")\n",
        "\n",
        "# Preprocess the data\n",
        "scam_data['input'] = scam_data['input'].str.replace(\"Classify this message:\", \"\", regex=True).str.strip().str.lower()\n",
        "\n",
        "# Validate preprocessing\n",
        "if scam_data['input'].isnull().any() or scam_data['output'].isnull().any():\n",
        "    raise ValueError(\"Null values found in 'input' or 'output' columns after preprocessing.\")\n",
        "\n",
        "# Split the data into training, validation, and test sets\n",
        "try:\n",
        "    train_data, temp_data = train_test_split(\n",
        "        scam_data, test_size=0.2, random_state=42, stratify=scam_data['output']\n",
        "    )\n",
        "    val_data, test_data = train_test_split(\n",
        "        temp_data, test_size=0.5, random_state=42, stratify=temp_data['output']\n",
        "    )\n",
        "except ValueError as e:\n",
        "    raise ValueError(f\"Error during train-test split: {e}\")\n",
        "\n",
        "# Output summary of splits\n",
        "print(f\"Training set size: {len(train_data)}\")\n",
        "print(f\"Validation set size: {len(val_data)}\")\n",
        "print(f\"Test set size: {len(test_data)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCs_EndLAXeo",
        "outputId": "98d28a42-c1d9-4830-8135-0a9ad89dfeba"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 4457\n",
            "Validation set size: 557\n",
            "Test set size: 558\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer\n",
        "\n",
        "# Tokenize data (Ensure these steps have been executed prior)\n",
        "def tokenize_data(inputs, targets, tokenizer, max_length=512):\n",
        "    encodings = tokenizer(inputs, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
        "    target_encodings = tokenizer(targets, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
        "    return encodings.input_ids, encodings.attention_mask, target_encodings.input_ids\n",
        "\n",
        "train_inputs, train_targets = format_data_for_t5(train_data)\n",
        "train_input_ids, train_attention_masks, train_labels = tokenize_data(train_inputs, train_targets, tokenizer)\n"
      ],
      "metadata": {
        "id": "Ql_9XhbIApPs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "from transformers import AdamW\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ti1yFptA6e_",
        "outputId": "989925dc-15fa-44f6-a9bb-37ab8298727e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.7169264756092187\n",
            "Epoch 2, Loss: 0.03167866690316543\n",
            "Epoch 3, Loss: 0.02116043094864702\n"
          ]
        }
      ]
    }
  ]
}